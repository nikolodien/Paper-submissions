\chapter{Experiments} % Main chapter title

\label{experiments} % For referencing the chapter elsewhere, use \ref{Chapter5} 

\lhead{Chapter 5. \emph{Experiments}} % This is for the header on each page - perhaps a shortened title

%----------------------------------------------------------------------------------------


In this chapter, we will show how \textit{LDA} was evaluated to test the accuracy of unsupervised approaches in general. Then, the use of topic
models to perform the sentiment classification task is explained. At the end, the system using deep semantics is explained.



In the next section, we explain the experiments done using deep semantics.

\section{Deep Semantics for sentiment analysis}

Analysis was performed for monolingual binary sentiment classification task. The language used in this case was \textit{English}. The comparison was done between 5 systems 
viz. System using words as features, WordNet sense based system as given in \citep*{balamurali2011harnessing}, Clusters based system as described in ~\citep*{arhaves}, 
Discourse rules based system as given in \citep*{mukherjee2012sentiment}, and the UNL rule based system. Two polarity datasets were used to perform the experiments. 
    
\begin{enumerate}
  \item \underline{EN-TD:} English Tourism corpus as used in \citep*{ye2009sentiment}. It consists of 594 positive and 593 negative reviews.
  \item \underline{EN-PD:} English Product (music albums) review corpus \citep*{blitzer2007biographies}. It consists of 702 positive and 702 negative 
  reviews. 
\end{enumerate}
   
For the WordNet sense, and Clusters based systems, a manually sense tagged version of the (EN-PD) has been used. Also, a automatically sense tagged version of 
(EN-TD) was used on these systems. The tagging in the later case was using an automated WSD engine, trained on a tourism domain \citep*{balamurali2013lost}.
The results reported for supervised systems are based on 10-fold cross validation.
   
\subsection*{Results}\label{results}

\begin{center}
  \begin{tabular}[h]{|l|c|c|}
   \hline
   \textbf{System} & \textbf{EN-TD} & \textbf{EN-PD} \\ \hline \hline
   Bag of Words & 85.53 & 73.24 \\ \hline
   Synset-based & 88.47 & 71.58 \\ \hline
   Cluster-based & \textbf{95.20} & 79.36 \\ \hline
   Discourse-based & 71.52 & 64.81 \\ \hline
   UNL rule-based & 86.08 & \textbf{79.55} \\ \hline
   \hline
  \end{tabular}
\end{center}
\begin{center}
 Table 7.10 Classification accuracy (in \%) for monolingual binary sentiment classification
\end{center}

  
The results for monolingual binary sentiment classification task are shown in Table ~\ref{table:accuracy}. The results reported are the best results obtained in 
case of supervised systems. The cluster based system performs the best in both cases. The UNL rule-based system performs better only than the bag of words
and discourse rule based system. For EN-PD ( music album reviews ) dataset, the UNL based system outperforms every other system . These results are very promising 
for a rule-based system. The difference between accuracy for positive and negative reviews for the rule-based systems viz. Discourse rules based and UNL rules based 
is shown in Table ~\ref{table:accuracyposneg}. It can be seen that the Discourse rules based system performs slightly better than the UNL based system for positive 
reviews. On the other hand, the UNL rules based system outperforms it in case of negative reviews by a huge margin. 
  
\begin{center}
  \begin{tabular}[h]{l|c|c|c|c|}
   \cline{2-5}
    & \multicolumn{2}{|c|}{\textbf{EN-TD}} & \multicolumn{2}{|c|}{\textbf{EN-PD}} \\ \hline
    \textbf{System} & \textbf{Pos} & \textbf{Neg} & \textbf{Pos} & \textbf{Neg} \\ \hline \hline
    Discourse rules & 94.94 & 48.06 & \textbf{92.73} & 36.89 \\ \hline
    UNL rules & \textbf{95.72} & \textbf{76.44} & 90.75 & \textbf{68.35} \\ \hline
   \hline
  \end{tabular}
\end{center} 
\begin{center}
 Table 7.11 Classification accuracy (in \%) for positive and negative reviews
\end{center}

\subsection*{Discussion}

The UNL generator used in this case is the bottleneck in terms of performance. Also, it makes use of the standard NLP tools viz. parsing, co-reference resolution, etc. 
to assign the proper semantic roles in the given \textit{text}. It is well known fact that these techniques work properly only on structured data. The language used in 
the reviews present in both the datasets is unstructured in considerable number of cases. The UNL generator is still in its infancy and cannot handle \textit{text} 
involving special characters. Due to these reasons, a proper UNL graph is not generated in some cases. Also, it is not able to generator proper UNL graphs for even well 
structured sentences like \textit{"It is not very good"}. In this case, the UW,  \textit{good} should have an attribute \textit{@not} which implies that it is used in 
a negative sense. On the contrary, the UNL generator does not assign any such attribute to it, leading to incorrect classification. As a result of all these things, the 
classification accuracy is low.

Negative reviews have certain characteristics which make them difficult to classify. Sentences like \textit{"It is good but could be better"} are very common in negative 
reviews. Some reviews are just to raise caution. A negative review about \textit{Las Vegas} contained the sentence, \textit{"Do not go to Las Vegas alone if you are female"}. 
Some reviews contain mixed opinions. For example, \textit{"Beautiful Architecture. Expensive food"}. We stated the importance of time in sentiment analysis in \cref{unl}. 
Examples of such a sentence is \textit{"A place I used to love. Not anymore"}. To infer that the sentence, \textit{"The hotel room had a Queen-sized bed"} has a negative 
sentiment is not possible without proper domain knowledge. Sentiment is domain dependent as given in ~\citep*{liu2010sentiment}. The adjective \textit{cheesy} might be positive for a 
food item but is definitely negative in cases like \textit{"The place is full of cheesy shows"}. Reviewers often criticize a place/thing by praising other places/things. In 
the EN-TD dataset, negative reviews were full of sentences like \textit{"If you want dirt, go to L.A. . If you want peace, go to Switzerland."}. \textit{"I love New York 
especially the 'Lovely brown fog'"} and \textit{"I adore the 'phony fantasy-land' that Vegas is"} are examples of sarcasm. Sarcasm is a very difficult problem to tackle. 
Some related works can be found in ~\citep*{carvalho2009clues} and ~\citep*{gonzalez2011identifying}.

In some cases, the reviewers make use of their native language and expressions. This is a big problem for the task of monolingual sentiment classification. From this discussion, it is clear that two major points of concern are unstructured language and hidden sentiment

\section*{Summary}

In this chapter, we first evaluated \textit{LDA} for the binary sentiment classification task. After that, we evaluated the topic models in different settings against
each other and the bag of words model. The system using deep semantics was compared with some state of the art systems for the same task but a different dataset.

In the next chapter, we conclude and comment on some future work.

\clearpage